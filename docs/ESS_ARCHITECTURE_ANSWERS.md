# ESS Architecture: Comprehensive Answers

**Date**: 2026-01-15
**Context**: Designing MCP-based project registration, Claude Code integration, and self-learning capabilities

---

## Question 1: How to Add a Project for Indexing

### Current State (Local Deployment Only)

**Existing workflow** (`veracity-engine/scripts/install.sh`):
```bash
cd ~/Projects/my-project
/path/to/veracity-engine/scripts/install.sh

# What happens:
# 1. Checks dependencies (Docker, Ollama)
# 2. Starts local Neo4j via docker compose
# 3. Creates Python venv in project directory
# 4. Runs build_graph.py â†’ indexes to LOCAL Neo4j
# 5. Installs git post-commit hook
# 6. Registers project in ~/.veracity/projects.yaml
```

**Limitation**: Only works for LOCAL Neo4j/Qdrant (same machine).

---

### Proposed Solution: MCP-Based Registration

#### Architecture

```
Developer Machine                          VPS (ess.ping-gadgets.com)
â”œâ”€ ess CLI tool                            â”œâ”€ Gateway (HTTPS)
â”‚  â””â”€ ess register my-project              â”‚  â””â”€ POST /api/mcp
â”‚     --root ~/Projects/my-project         â”‚
â”‚     --target-dirs src tests docs         â”œâ”€ MCP Handler
â”‚                                           â”‚  â””â”€ Forward to veracity MCP
â”‚                                           â”‚
â”‚                                           â”œâ”€ Veracity MCP Server
â”‚                                           â”‚  â””â”€ register_project(...)
â”‚                                           â”‚  â””â”€ index_project(...)
â”‚                                           â”‚
â”‚                                           â””â”€ Neo4j + Qdrant
```

#### Implementation

**Step 1: Add MCP Tools to Veracity Engine**

Edit `veracity-engine/core/mcp_server.py`:

```python
# Add to TOOLS array
Tool(
    name="register_project",
    description="""Register a new project for indexing.

    Creates project configuration and prepares for initial indexing.
    Does NOT index immediately - use index_project for that.""",
    inputSchema={
        "type": "object",
        "properties": {
            "project_name": {"type": "string"},
            "root_dir": {"type": "string", "description": "Project root directory on VPS"},
            "target_dirs": {
                "type": "array",
                "items": {"type": "string"},
                "description": "Directories to index (e.g., ['src', 'tests', 'docs'])"
            },
            "watch_mode": {
                "type": "string",
                "enum": ["realtime", "polling", "git-only"],
                "default": "realtime"
            }
        },
        "required": ["project_name", "root_dir"]
    }
),

Tool(
    name="index_project",
    description="""Index or re-index a project into the knowledge graph.

    Runs full AST parsing, embedding generation, and graph creation.
    Can be called for initial indexing or updates.""",
    inputSchema={
        "type": "object",
        "properties": {
            "project_name": {"type": "string"},
            "incremental": {
                "type": "boolean",
                "default": True,
                "description": "Only index changed files (default: true)"
            },
            "force": {
                "type": "boolean",
                "default": False,
                "description": "Force full re-index (default: false)"
            }
        },
        "required": ["project_name"]
    }
),

Tool(
    name="ingest_files",
    description="""Ingest specific files into the knowledge graph.

    For real-time updates when files change locally.
    More efficient than full project re-indexing.""",
    inputSchema={
        "type": "object",
        "properties": {
            "project_name": {"type": "string"},
            "files": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "path": {"type": "string"},
                        "content": {"type": "string"},
                        "language": {"type": "string"}
                    }
                }
            }
        },
        "required": ["project_name", "files"]
    }
)
```

**Tool Handlers**:

```python
@server.call_tool()
async def call_tool(name: str, arguments: dict) -> list[TextContent]:
    if name == "register_project":
        from core.project_registry import register_project_config
        project_name = arguments["project_name"]
        root_dir = arguments["root_dir"]
        target_dirs = arguments.get("target_dirs", ["src", "tests", "docs"])
        watch_mode = arguments.get("watch_mode", "realtime")

        # Register in ~/.veracity/projects.yaml
        register_project_config(
            name=project_name,
            root_dir=root_dir,
            target_dirs=target_dirs,
            watch_mode=watch_mode
        )

        return [TextContent(
            type="text",
            text=f"âœ… Project '{project_name}' registered\nReady for indexing with: index_project"
        )]

    elif name == "index_project":
        from core.build_graph import index_project
        project_name = arguments["project_name"]
        incremental = arguments.get("incremental", True)
        force = arguments.get("force", False)

        # Run indexing (calls existing build_graph.py logic)
        result = await index_project(
            project_name=project_name,
            incremental=incremental,
            force=force
        )

        return [TextContent(
            type="text",
            text=f"âœ… Indexed {result['files_processed']} files\n"
                 f"Created {result['nodes_created']} nodes\n"
                 f"Time: {result['duration_ms']}ms"
        )]

    elif name == "ingest_files":
        from core.file_ingestion import ingest_changed_files
        project_name = arguments["project_name"]
        files = arguments["files"]

        # Ingest files directly (for real-time updates)
        result = await ingest_changed_files(
            project_name=project_name,
            files=files
        )

        return [TextContent(
            type="text",
            text=f"âœ… Ingested {len(files)} files"
        )]
```

**Step 2: Expose MCP via Gateway**

Edit `gateway/src/server.ts`:

```typescript
// Add MCP endpoint
app.post('/api/mcp', authenticate, async (req, res) => {
  const { tool, arguments: args } = req.body;

  // Forward to veracity MCP server (via stdio or HTTP)
  const result = await veracityMCP.callTool(tool, args);

  res.json(result);
});
```

**Step 3: Create CLI Client**

`ess-cli/bin/ess.js`:

```javascript
#!/usr/bin/env node
import { Command } from 'commander';
import axios from 'axios';

const program = new Command();

program
  .name('ess')
  .description('ESS CLI - Manage projects and query knowledge graph')
  .version('1.0.0');

program
  .command('register <project>')
  .description('Register a project for indexing')
  .option('-r, --root <dir>', 'Project root directory')
  .option('-t, --target-dirs <dirs>', 'Directories to index (comma-separated)')
  .option('-w, --watch-mode <mode>', 'Watch mode (realtime|polling|git-only)', 'realtime')
  .action(async (project, options) => {
    const targetDirs = options.targetDirs?.split(',') || ['src', 'tests', 'docs'];

    const response = await axios.post('https://ess.ping-gadgets.com/api/mcp', {
      tool: 'register_project',
      arguments: {
        project_name: project,
        root_dir: options.root || process.cwd(),
        target_dirs: targetDirs,
        watch_mode: options.watchMode
      }
    }, {
      headers: {
        'Authorization': `Bearer ${process.env.ESS_API_KEY}`
      }
    });

    console.log(response.data.content[0].text);
  });

program
  .command('index <project>')
  .description('Index a project')
  .option('-f, --force', 'Force full re-index')
  .action(async (project, options) => {
    const response = await axios.post('https://ess.ping-gadgets.com/api/mcp', {
      tool: 'index_project',
      arguments: {
        project_name: project,
        force: options.force || false
      }
    }, {
      headers: {
        'Authorization': `Bearer ${process.env.ESS_API_KEY}`
      }
    });

    console.log(response.data.content[0].text);
  });

program
  .command('query <project> <question>')
  .description('Query the knowledge graph')
  .action(async (project, question) => {
    const response = await axios.post('https://ess.ping-gadgets.com/api/mcp', {
      tool: 'query_codebase',
      arguments: {
        project_name: project,
        question: question
      }
    }, {
      headers: {
        'Authorization': `Bearer ${process.env.ESS_API_KEY}`
      }
    });

    console.log(response.data.content[0].text);
  });

program.parse();
```

**Step 4: Local File Watcher â†’ MCP Sync**

`ess-watcher/watch.js`:

```javascript
import chokidar from 'chokidar';
import axios from 'axios';
import fs from 'fs';

const projectName = process.env.ESS_PROJECT_NAME;
const watchDirs = process.env.ESS_WATCH_DIRS.split(',');

const watcher = chokidar.watch(watchDirs, {
  ignores: /node_modules|\.git|dist/,
  persistent: true
});

watcher.on('change', async (filePath) => {
  console.log(`File changed: ${filePath}`);

  const content = fs.readFileSync(filePath, 'utf-8');
  const language = detectLanguage(filePath);

  await axios.post('https://ess.ping-gadgets.com/api/mcp', {
    tool: 'ingest_files',
    arguments: {
      project_name: projectName,
      files: [{
        path: filePath,
        content: content,
        language: language
      }]
    }
  }, {
    headers: {
      'Authorization': `Bearer ${process.env.ESS_API_KEY}`
    }
  });

  console.log(`âœ… Synced ${filePath} to ESS`);
});
```

---

### Usage Workflow

#### Initial Setup (One-Time)

```bash
# Install ESS CLI
npm install -g @ess/cli

# Set API key
export ESS_API_KEY="your-api-key"

# Register project (creates remote config)
ess register ping-learn-pwa \
  --root ~/Projects/ping-learn-pwa \
  --target-dirs app/src,app/prisma,docs \
  --watch-mode realtime

# Initial indexing (VPS runs build_graph.py)
ess index ping-learn-pwa

# Expected output:
# âœ… Indexed 530 files
# Created 2,000 nodes
# Time: 45,000ms
```

#### Continuous Sync (Automated)

**Option A: Local File Watcher**

```bash
cd ~/Projects/ping-learn-pwa

# Start watcher (runs in background)
ess watch --daemon

# Now every file save â†’ auto-sync to VPS
```

**Option B: Git Hook**

```bash
# .git/hooks/post-commit
#!/bin/bash
ess index ping-learn-pwa --incremental
```

**Option C: Claude Code Integration** (see Question 2)

---

### Summary

| Method | Setup | Sync Speed | Use Case |
|--------|-------|------------|----------|
| **MCP CLI** | `ess register` + `ess index` | Manual | Initial setup |
| **File Watcher** | `ess watch --daemon` | <1s (real-time) | Active development |
| **Git Hook** | Add to `.git/hooks/post-commit` | 5-10s | Post-commit |
| **Claude Code** | Built-in MCP | <1s | AI agent queries |

**Recommended**: File Watcher for development, Git Hook for commits, MCP for queries.

---

## Question 2: Claude Code + ESS Integration Strategy

### Current State

**Existing MCP Server**:
- Location: `veracity-engine/core/mcp_server.py`
- Tools: `query_codebase`, `get_component_map`, `list_projects`, `get_file_relationships`
- **Limitation**: Configured for LOCAL Neo4j only

**Installation** (`veracity-engine/scripts/install-mcp.sh`):
```bash
# Adds to ~/.config/claude-code/config.json
{
  "mcpServers": {
    "veracity": {
      "command": "/opt/homebrew/bin/python3.11",
      "args": ["/path/to/veracity-engine/core/mcp_server.py"],
      "env": {
        "NEO4J_URI": "bolt://localhost:7687",  # âŒ LOCAL ONLY
        "NEO4J_USER": "neo4j",
        "NEO4J_PASSWORD": "password"
      }
    }
  }
}
```

---

### Proposed Architecture: Hybrid Local + Remote

#### Design Principle

**Primary**: Fast local search (Grep/Glob/Read)
**Secondary**: ESS structural context (when needed)
**Hybrid**: Combine for complete understanding

#### Implementation Strategy

**Scenario 1: Simple Questions** (90% of queries)

```
User: "Where is the FallbackManager class?"

Claude Code:
1. Grep: "class FallbackManager" (100ms)
2. Found: knowledge-base/src/core/FallbackManager.ts:15
3. Answer immediately (no ESS needed)
```

**Scenario 2: Architectural Questions** (10% of queries)

```
User: "How does the FallbackManager integrate with the gateway?"

Claude Code:
1. Local Grep: Find FallbackManager (100ms)
2. ESS MCP: Get relationships (200ms)
   â†’ query_codebase("How is FallbackManager used in gateway?")
   â†’ Returns: gateway/src/agents/EnggContextAgent.ts:45 (CALLS)
3. Local Read: Read both files (50ms)
4. Answer with full context (total: 350ms)
```

**Scenario 3: Cross-Project Questions**

```
User: "How should I implement auth in my new project?"

Claude Code:
1. ESS MCP: query_codebase("auth implementations")
   â†’ Finds: ping-learn-pwa uses NextAuth, ess uses API keys
2. context7: Search latest React auth patterns
3. Synthesize: Combine ESS examples + web best practices
```

---

### Configuration: Remote ESS MCP

**Option A: HTTP Transport** (Recommended for VPS)

Edit `~/.config/claude-code/config.json`:

```json
{
  "mcpServers": {
    "veracity-remote": {
      "command": "node",
      "args": ["/path/to/ess-mcp-client.js"],
      "env": {
        "ESS_API_URL": "https://ess.ping-gadgets.com/api/mcp",
        "ESS_API_KEY": "your-api-key"
      }
    }
  }
}
```

**MCP HTTP Client** (`ess-mcp-client.js`):

```javascript
#!/usr/bin/env node
import { Server } from '@modelcontextprotocol/sdk/server/index.js';
import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';
import axios from 'axios';

const server = new Server({ name: 'veracity-remote', version: '1.0.0' });

// Forward all tool calls to VPS gateway
server.setRequestHandler('tools/call', async (request) => {
  const { name, arguments: args } = request.params;

  const response = await axios.post(process.env.ESS_API_URL, {
    tool: name,
    arguments: args
  }, {
    headers: {
      'Authorization': `Bearer ${process.env.ESS_API_KEY}`
    }
  });

  return response.data;
});

// List available tools (query VPS)
server.setRequestHandler('tools/list', async () => {
  const response = await axios.get(
    `${process.env.ESS_API_URL}/tools`,
    {
      headers: {
        'Authorization': `Bearer ${process.env.ESS_API_KEY}`
      }
    }
  );
  return { tools: response.data.tools };
});

const transport = new StdioServerTransport();
await server.connect(transport);
```

**Option B: SSH Tunnel** (For direct Neo4j access)

```json
{
  "mcpServers": {
    "veracity-tunnel": {
      "command": "/opt/homebrew/bin/python3.11",
      "args": ["/path/to/veracity-engine/core/mcp_server.py"],
      "env": {
        "NEO4J_URI": "bolt://localhost:7688",  # Tunneled port
        "NEO4J_USER": "neo4j",
        "NEO4J_PASSWORD": "password123"
      }
    }
  }
}
```

Then create SSH tunnel:
```bash
ssh -L 7688:ess-neo4j:7687 root@ess.ping-gadgets.com -N -f
```

---

### Usage in Claude Code

**Automatic (Built-in Routing)**

Claude Code automatically uses MCP tools when relevant:

```
User: "Show me how ping-learn handles authentication"

[Claude Code thinks]
1. This is a cross-component question â†’ use ESS
2. Call: veracity-remote:query_codebase(
     project_name="ping-learn-pwa",
     question="authentication implementation"
   )
3. ESS returns: [evidence with file:line citations]
4. Claude reads those specific files locally
5. Synthesizes answer with full context
```

**Manual (Explicit Invocation)**

```
User: "use veracity to find all React hooks in ping-learn"

Claude Code:
â†’ veracity-remote:query_codebase(
    project_name="ping-learn-pwa",
    question="list all custom React hooks"
  )
```

---

### Agent SDK Integration

For custom agents that need deterministic ESS access:

```typescript
// agents/ess-aware-agent.ts
import { Agent } from '@anthropic-ai/sdk/agents';
import axios from 'axios';

export class ESSAwareAgent extends Agent {
  async queryCodebase(project: string, question: string) {
    // Query ESS for structural context
    const response = await axios.post(
      'https://ess.ping-gadgets.com/api/mcp',
      {
        tool: 'query_codebase',
        arguments: { project_name: project, question }
      },
      {
        headers: {
          'Authorization': `Bearer ${process.env.ESS_API_KEY}`
        }
      }
    );

    return response.data.content[0].text;
  }

  async run(task: string) {
    // 1. Get architectural context from ESS
    const context = await this.queryCodebase('ping-learn-pwa', task);

    // 2. Use context to inform local code search
    const files = this.extractFileReferences(context);
    const code = await this.readFiles(files);

    // 3. Combine ESS graph + local code â†’ complete understanding
    return this.synthesize(context, code);
  }
}
```

---

### Determinism Strategy

**Key Principle**: ESS provides FACTS, not LLM synthesis

1. **ESS Returns**:
   - File paths with line numbers
   - Relationships (CALLS, IMPORTS, DEFINES)
   - Veracity scores (staleness, orphans)
   - Raw code snippets

2. **Claude Code Uses**:
   - ESS facts as evidence
   - Local reads for full context
   - LLM synthesis ONLY after gathering all evidence

3. **Verification**:
   - Every claim cites `file:line`
   - Veracity scores inform confidence
   - If score <0.5 â†’ warn user

---

### Summary

| Search Type | Tool | Speed | Use Case |
|-------------|------|-------|----------|
| **File location** | Local Grep | 100ms | "Where is X?" |
| **Code content** | Local Read | 50ms | "Show me X" |
| **Relationships** | ESS MCP | 200ms | "How does X use Y?" |
| **Architecture** | ESS + Local | 350ms | "Explain the auth flow" |
| **Cross-project** | ESS + context7 | 500ms | "Compare implementations" |

**Recommendation**: Keep local search as primary, use ESS for architectural context.

---

## Question 3: Self-Learning & Self-Improving Capabilities

### What EXISTS (Gateway)

#### 1. QueryMetricsStore (`gateway/src/metrics/QueryMetrics.ts`)

**Purpose**: Feedback loop for query performance

**Features**:
- Logs every query with metrics:
  - `semanticMatchCount` (Qdrant results)
  - `structuralMatchCount` (Neo4j results)
  - `avgSemanticScore` (quality of matches)
  - `confidence` (calculated score 0-1)
  - `latencyMs` (performance)
  - `citationCount` (evidence quality)

- Captures user feedback:
  - `useful` / `not_useful` / `partial`
  - Optional comments
  - Timestamp

- Stores in Redis:
  - 7-day TTL (configurable)
  - Indexed by `requestId`
  - Falls back to in-memory if Redis unavailable

**Storage Format**:
```json
{
  "requestId": "req_abc123",
  "timestamp": "2026-01-15T10:30:00Z",
  "query": "How does FallbackManager work?",
  "queryHash": "a3f4b2c1",
  "semanticMatchCount": 12,
  "structuralMatchCount": 5,
  "avgSemanticScore": 0.85,
  "confidence": 0.78,
  "answerLength": 450,
  "citationCount": 3,
  "latencyMs": 250,
  "feedback": "useful",
  "feedbackTimestamp": "2026-01-15T10:35:00Z",
  "feedbackComment": "Exactly what I needed"
}
```

**API**:
- `log(metric)` - Store query metric
- `updateFeedback(requestId, feedback, comment)` - Add user feedback
- `getMetricsWithFeedback(limit)` - Get queries with feedback
- `getSummary(sinceDays)` - Get aggregate stats

#### 2. ConfidenceTuner (`gateway/src/metrics/ConfidenceTuner.ts`)

**Purpose**: Machine learning for confidence score optimization

**Algorithm**:
```
1. Load metrics with feedback (last 7 days)
2. Separate: useful vs not_useful responses
3. For each factor (semanticScore, structuralPresence, citationCoverage):
   a. Compute: avg(factor | useful) - avg(factor | not_useful)
   b. Calculate correlation: diff / max(avg)
   c. Suggest adjustment: correlation * 0.1
4. Apply adjustments to weights (bounded 0.05-0.9)
5. Normalize weights to sum = 1.0
6. If confidence > 0.8 AND samples >= 10 â†’ auto-apply
   Else â†’ save for human review
```

**Weight Optimization**:
```json
{
  "version": 2,
  "updatedAt": "2026-01-15T10:00:00Z",
  "updatedBy": "confidence-tuner",
  "weights": {
    "semanticScore": 0.65,      // Down from 0.7 (less useful)
    "structuralPresence": 0.20,  // Up from 0.1 (more useful)
    "citationCoverage": 0.15     // Down from 0.2
  },
  "thresholds": {
    "high": 0.8,
    "medium": 0.5,
    "low": 0.3
  },
  "behavior": {
    "belowLow": "warn",
    "belowMedium": "include_raw"
  }
}
```

**Correlation Output**:
```json
{
  "factor": "structuralPresence",
  "usefulCorrelation": 0.72,
  "avgWhenUseful": 0.85,
  "avgWhenNotUseful": 0.23,
  "suggestedAdjustment": +0.072
}
```

**Cron Integration**:
```bash
# Run tuning daily
0 2 * * * cd /path/to/gateway && node dist/metrics/ConfidenceTuner.js
```

---

### What EXISTS (Veracity Engine)

#### 3. Veracity Scoring (`veracity-engine/core/ask_codebase.py`)

**Purpose**: Detect stale/unreliable graph data

**Rules**:
- **STALE_DOC**: Document >90 days old â†’ -15 confidence points
- **ORPHANED_NODE**: Node with <2 connections â†’ -5 points
- **CONTRADICTION**: (Placeholder) Doc/code timestamp mismatch â†’ -20 points

**Implementation**:
```python
def calculate_veracity(node, relationships):
    score = 100  # Base confidence

    # Check staleness
    if node.last_modified:
        days_old = (datetime.now() - node.last_modified).days
        if days_old > 90:
            score -= 15

    # Check orphans
    if len(relationships) < 2:
        score -= 5

    # Check contradictions (TODO: implement)
    # if doc_timestamp != code_timestamp:
    #     score -= 20

    return max(0, score)
```

---

### What's MISSING (Critical Gaps)

#### 4. Embedding Refinement (Not Implemented)

**Problem**: Embeddings are static - never improve with usage

**Proposed Solution**: Query-aware embedding tuning

```python
# embedding-tuner/refine.py
class EmbeddingRefiner:
    """Refine embeddings based on query feedback."""

    def refine_embeddings(self, feedback_data):
        """
        1. Load queries with "useful" feedback
        2. Extract common query patterns
        3. Fine-tune embedding model on positive examples
        4. Re-generate embeddings for affected nodes
        5. A/B test: old vs new embeddings
        6. If improvement > 5% â†’ deploy new embeddings
        """

        # Collect positive examples
        useful_queries = [
            q for q in feedback_data
            if q['feedback'] == 'useful'
            and q['avgSemanticScore'] > 0.8
        ]

        # Extract query-document pairs
        training_pairs = []
        for query in useful_queries:
            for result in query['results']:
                training_pairs.append({
                    'query': query['text'],
                    'positive_doc': result['content'],
                    'score': result['score']
                })

        # Fine-tune nomic-embed-text
        model = load_model('nomic-embed-text')
        model.fine_tune(training_pairs, epochs=5)

        # Re-embed high-traffic nodes
        high_traffic_nodes = get_frequently_queried_nodes(days=30)
        for node in high_traffic_nodes:
            new_embedding = model.encode(node.content)
            node.embedding = new_embedding
            db.update(node)
```

**Storage**: New embeddings in separate Qdrant collection for A/B testing

#### 5. Pattern Recognition (Not Implemented)

**Problem**: System doesn't learn from repeated queries

**Proposed Solution**: Query pattern detector

```python
class PatternDetector:
    """Detect common query patterns and pre-compute answers."""

    def detect_patterns(self, metrics):
        """
        1. Cluster similar queries (embeddings + edit distance)
        2. Identify frequent patterns (>10 occurrences)
        3. Pre-compute optimal results for patterns
        4. Cache in Redis with 7-day TTL
        """

        from sklearn.cluster import DBSCAN

        # Embed all queries
        query_embeddings = [
            embed(m['query']) for m in metrics
        ]

        # Cluster
        clustering = DBSCAN(eps=0.3, min_samples=10)
        labels = clustering.fit_predict(query_embeddings)

        # Extract frequent patterns
        for label in set(labels):
            if label == -1:  # Noise
                continue

            cluster_queries = [
                metrics[i] for i, l in enumerate(labels) if l == label
            ]

            # Compute canonical query for cluster
            canonical = compute_centroid(cluster_queries)

            # Pre-compute optimal answer
            answer = precompute_answer(canonical)

            # Cache
            redis.setex(
                f"pattern:{label}",
                7 * 24 * 60 * 60,  # 7 days
                json.dumps(answer)
            )
```

**Usage**: When new query arrives, check if it matches a pattern â†’ return cached answer (latency: 10ms vs 200ms)

#### 6. Code Change Tracking (Not Implemented)

**Problem**: No learning from how code evolves

**Proposed Solution**: Diff-based learning

```python
class CodeEvolutionTracker:
    """Learn from code changes over time."""

    def track_changes(self, git_diff):
        """
        1. Parse git diff for changed files
        2. Categorize changes (refactor, feature, bugfix)
        3. Update graph relationships
        4. Learn patterns (e.g., "auth changes often touch 3 files")
        """

        for change in parse_diff(git_diff):
            file_path = change.path
            change_type = classify_change(change)  # refactor|feature|bug

            # Update node metadata
            node = graph.get_node(file_path)
            node.metadata['last_change_type'] = change_type
            node.metadata['change_frequency'] += 1

            # Learn co-change patterns
            if change_type == 'feature':
                co_changed_files = find_co_changed_files(git_log, file_path)
                for related_file in co_changed_files:
                    graph.add_relationship(
                        file_path,
                        related_file,
                        'CO_CHANGES_WITH',
                        weight=calculate_co_change_strength(git_log)
                    )
```

**Value**: Predict "If you change X, you probably need to change Y too"

#### 7. Error Correction (Not Implemented)

**Problem**: When ESS gives wrong answers, it doesn't learn

**Proposed Solution**: Correction feedback loop

```python
class ErrorCorrector:
    """Learn from corrections."""

    def record_correction(self, original_query, wrong_answer, correct_answer):
        """
        1. Store correction in corrections table
        2. Analyze why the wrong answer was returned
        3. Adjust graph weights to prevent recurrence
        """

        # Store correction
        db.corrections.insert({
            'query': original_query,
            'wrong_answer': wrong_answer,
            'correct_answer': correct_answer,
            'timestamp': datetime.now()
        })

        # Analyze causation
        wrong_nodes = extract_nodes_from_answer(wrong_answer)
        correct_nodes = extract_nodes_from_answer(correct_answer)

        # Penalize wrong nodes for this query pattern
        for node in wrong_nodes:
            node.query_penalties[hash(original_query)] = -0.2

        # Boost correct nodes
        for node in correct_nodes:
            node.query_bonuses[hash(original_query)] = +0.2
```

**Usage**: Next similar query â†’ boosted nodes rank higher

---

### Proposed Self-Learning Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Self-Learning ESS                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Query Metricsâ”‚â”€â”€â”€â–¶â”‚Confidence    â”‚â”€â”€â”€â–¶â”‚ Weight       â”‚ â”‚
â”‚  â”‚   Store      â”‚    â”‚  Tuner       â”‚    â”‚ Optimizer    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                     â”‚                     â”‚        â”‚
â”‚         â–¼                     â–¼                     â–¼        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Pattern      â”‚    â”‚ Embedding    â”‚    â”‚ Error        â”‚ â”‚
â”‚  â”‚ Detector     â”‚    â”‚  Refiner     â”‚    â”‚ Corrector    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                     â”‚                     â”‚        â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                               â”‚                              â”‚
â”‚                               â–¼                              â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚                    â”‚ Knowledge Graph  â”‚                      â”‚
â”‚                    â”‚ (Neo4j + Qdrant) â”‚                      â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Implementation Roadmap

#### Phase 1: Foundational Self-Learning (Weeks 1-2)

**Goal**: Deploy existing systems + add feedback UI

- [x] QueryMetricsStore (EXISTS)
- [x] ConfidenceTuner (EXISTS)
- [ ] Add feedback UI to gateway
  - Thumbs up/down on every response
  - Optional comment field
  - API: `POST /api/feedback/:requestId`
- [ ] Set up cron job for tuning
  - Daily at 2 AM: `node dist/metrics/ConfidenceTuner.js`
- [ ] Dashboard for metrics
  - View feedback summary
  - See weight evolution over time
  - Compare confidence before/after tuning

**Expected Improvement**: 10-15% better confidence scores after 2 weeks

#### Phase 2: Pattern Recognition (Weeks 3-4)

**Goal**: Pre-compute answers for frequent queries

- [ ] Implement PatternDetector
- [ ] Add pattern cache to Redis
- [ ] Create pattern dashboard
  - Show top 10 query patterns
  - Show cache hit rate
  - Manual pattern curation

**Expected Improvement**: 50% faster for top 20% of queries

#### Phase 3: Embedding Refinement (Weeks 5-8)

**Goal**: Fine-tune embeddings based on usage

- [ ] Implement EmbeddingRefiner
- [ ] Set up A/B testing infrastructure
  - Old embeddings vs new embeddings
  - Measure: avgSemanticScore improvement
- [ ] Auto-deploy if improvement > 5%

**Expected Improvement**: 15-20% better semantic scores

#### Phase 4: Code Evolution Tracking (Weeks 9-12)

**Goal**: Learn from git history

- [ ] Implement CodeEvolutionTracker
- [ ] Add git webhook integration
  - On push â†’ analyze changes
  - Update co-change relationships
- [ ] Add "Related files you might need" feature

**Expected Improvement**: Proactive suggestions reduce back-and-forth

#### Phase 5: Error Correction (Weeks 13-16)

**Goal**: Self-correct from mistakes

- [ ] Implement ErrorCorrector
- [ ] Add "Report wrong answer" UI
- [ ] Auto-adjust weights based on corrections

**Expected Improvement**: Decreasing error rate over time

---

### Success Metrics

**Week 0 (Baseline)**:
- Confidence accuracy: 65%
- Avg latency: 200ms
- User satisfaction: Unknown

**Week 16 (Target)**:
- Confidence accuracy: 85%
- Avg latency: 100ms (50% from caching)
- User satisfaction: 80% "useful" feedback
- Error rate: <5% (down from ~15%)

---

### Key Insights

1. **ESS already has strong self-learning foundation** (QueryMetrics + ConfidenceTuner)
2. **Missing pieces are HIGHER-ORDER learning** (patterns, embeddings, evolution)
3. **All improvements are DETERMINISTIC** (no black-box ML, only correlation-based tuning)
4. **Feedback loop closes in days, not months** (7-day metrics window)

---

## Summary & Next Actions

### Question 1: Project Registration
- âœ… **Solution**: MCP-based registration via `register_project`, `index_project`, `ingest_files` tools
- ðŸŽ¯ **Next**: Implement 3 MCP tools in veracity-engine, expose via gateway

### Question 2: Claude Code Integration
- âœ… **Solution**: Hybrid local (Grep/Read) + remote ESS (MCP for architecture)
- ðŸŽ¯ **Next**: Create MCP HTTP client, configure remote endpoint

### Question 3: Self-Learning
- âœ… **Exists**: QueryMetrics, ConfidenceTuner (solid foundation)
- âš ï¸ **Missing**: Pattern recognition, embedding refinement, error correction
- ðŸŽ¯ **Next**: Deploy feedback UI, implement PatternDetector (Phase 2)

---

**Created**: 2026-01-15
**Status**: Ready for implementation
**Priority**: Question 1 (registration) â†’ Question 2 (integration) â†’ Question 3 (self-learning phases)
